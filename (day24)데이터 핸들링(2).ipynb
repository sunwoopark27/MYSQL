# 데이터 수집 방법
# - 웹 클롤링
# - API 를 통한 데이터 수집


#  1. 웹 크롤링
#     웹 크롤러 또는 스파이더 프로그램을 활용하여 인터넷 상의 웹페이지 탐색하고 데이터 수집하는 기술
#     웹 스크래핑
#     크롤러와 조금 다르게 특정한 정보를 추출하여 가공하는 기술
#     웹 주소에 robots.text 붙이면 값조회 

#    크롤링의 특징
#     - 자동화
#     - 대량 데이터 수집
#     - 구조화된 데이터 추출

#     작동원리
#     - HTTP 요청 : 웹페이지에 접근
#     - HTML 응답 : 서버로 부터 html코드 포함한 응답 수신
#     - HTML 파싱 : 라이브러리를 활용하여 HTML 구조 분석 및 데이터 추출
#     - 데이터 저장 및 분석 : 추출한 데이터를 CSV, JSON, XML 등으로 저장하여 분석에 활용
 
#    ** 크롤링시 고려사항
#    - 대량의 요청을 보내면 웹 서버에 과부하를 유발할 수 있어 서비스 장애 유발 ➡ 적절한 요청 간격을 두어 수행해야 함
#    - 사용자 동의없이 개인정보가 수집될 수 있는 경우 개인정보보호법 위반
#    - 크롤링이 가능한 사이트인지 이용약관을 확인 후 수집
#    - 무단으로 수집하여 상업적 목적으로 사용하는 경우 저작권법 위반

#    크롤링을 위한 파이썬 라이브러리
#     1. Requests : 단순한 형태의 문법, 인코딩 확인, 헤더 파악, 텍스트 전환이 용이하고, 딕셔너리 형태로 데이터 전송 (HTTP 요청, 응답)
#     2. BeautifulSoup : (동기요청 :하나의 응답을 보내면 그 응답이 오고 나서 보냄)HTML 문법으로 작성된 문서를 파싱하여 필요한 부분의 태그를 데이터 추출 및 정제 (파싱을 위한 도구)
#     3. Scarpy : 비동기 요청(Asynchronous Requests)(응답올 때까지 기다리지 않음)을 지원하므로 매우 빠르게 동작하여 데이터 수집, 변환, #저장
#     4. Selenium : 웹브라우저(크롬 등) 드라이버 설치 필요, 동적인 여러가지 상호작용으로 데이터 수집
#                   정적 웹 : 사이트 안변함 vs 동적 웹 : 사이트 새로고침 할 때 바뀜 ex) 세계 시간


#     ** REQUEST
#       : 텍스트 형태로 받아옴
#         요청과 응답 처리가 간단하여 가독성 높음
#         데이터 다운로드 / API 호출에 적합

####  - GET 메소드와 URL

# 웹통신 라이브러리 requests 설치 **
!pip install requests

import requests

# HTTP 요청
response = requests.get("https://www.naver.com")
# 응답의 본문(text) 출력
print(response.text)

